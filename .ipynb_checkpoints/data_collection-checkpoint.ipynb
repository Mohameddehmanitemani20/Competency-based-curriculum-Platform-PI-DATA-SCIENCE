{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7cd3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaca8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"C:/Users/MSI/Desktop/Projet Data Science PI principal/Data Collection/Journée Entreprise (réponses) - Réponses au formulaire 1 (1).csv\")\n",
    "df2=pd.read_csv(\"C:/Users/MSI/Desktop/Projet Data Science PI principal/Data Collection/Journée Entreprise (réponses) - Réponses au formulaire 1.csv\")\n",
    "df3=pd.read_csv(\"C:/Users/MSI/Desktop/Projet Data Science PI principal/Data Collection/Competency Based Curriculum 4DS4 - Réponses au formulaire 1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee27d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"4DS6\",df1\n",
    "        ,\"4DS8\",df2\n",
    "        ,\"4DS4\",df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae49b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataP=pd.concat([df1,df2,df3])\n",
    "dataP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9481e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataP.drop('Score', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d30f6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18f7c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50285386",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataP.to_csv(\"data2.csv\",index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7fbf3d",
   "metadata": {},
   "source": [
    "## web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fe347d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cloudscraper in c:\\users\\msi\\anaconda3\\lib\\site-packages (1.2.68)\n",
      "Requirement already satisfied: requests-toolbelt>=0.9.1 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from cloudscraper) (0.10.1)\n",
      "Requirement already satisfied: pyparsing>=2.4.7 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from cloudscraper) (2.4.7)\n",
      "Requirement already satisfied: requests>=2.9.2 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from cloudscraper) (2.25.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (2022.12.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install cloudscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ab60020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import cloudscraper\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "047c85ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(page):\n",
    "    headers={'User-Agent':  'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}\n",
    "    #scraper = cloudscraper.create_scraper(delay=10,   browser={'custom': 'ScraperBot/1.0',})\n",
    "    url=f'https://www.indeed.com/jobs?q=data+scientist&start={page}'\n",
    "    r= requests.get(url,headers)\n",
    "    return r.status_code\n",
    "    #soup =BeautifulSoup(r.content,'html.parser')\n",
    "    #return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f23a480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403\n"
     ]
    }
   ],
   "source": [
    "print(extract(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563132d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=extract(30)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cec4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(soup):\n",
    "    divs=soup.find_all('div',class_='cardOutline tapItem ')\n",
    "    return len(divs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7988a9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transform(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f288c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7c29c7a",
   "metadata": {},
   "source": [
    "### scraping with selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49074288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from time import time\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f0b3ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#########################################\n",
      "#           WEBSITE: INDEED.FR          #\n",
      "######################################### \n",
      "\n"
     ]
    }
   ],
   "source": [
    "website = \"\"\"\n",
    "#########################################\n",
    "#           WEBSITE: INDEED.FR          #\n",
    "######################################### \n",
    "\"\"\"\n",
    "print(website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3411941d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawl starting time : 00:30:11.386225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "print('Crawl starting time : {}' .format(start_time.time()))\n",
    "print()\n",
    "job_list = [\"data+analyst\", \"data+scientist\", \"business+analyst\"]\n",
    "\n",
    "application_links = []\n",
    "job_titles = []\n",
    "company_names = []\n",
    "job_locations = []\n",
    "application_types = []\n",
    "publication_dates = []\n",
    "scraping_dates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d3371364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-44-25a77ea31137>:4: DeprecationWarning: headless property is deprecated, instead use add_argument('-headless')\n",
      "  opts.headless = True\n",
      "<ipython-input-44-25a77ea31137>:5: DeprecationWarning: firefox_profile has been deprecated, please use an Options object\n",
      "  profile = webdriver.FirefoxProfile('./firefoxdriver')\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Le chemin d’accès spécifié est introuvable: './firefoxdriver'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-25a77ea31137>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mopts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFirefoxOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mopts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheadless\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mprofile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFirefoxProfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./firefoxdriver'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprofile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_preference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"general.useragent.override\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\firefox\\firefox_profile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, profile_directory)\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtempfolder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdtemp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mnewprof\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtempfolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"webdriver-py-profilecopy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             shutil.copytree(\n\u001b[0m\u001b[0;32m     74\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewprof\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_patterns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"parent.lock\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lock\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\".parentlock\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[1;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[0;32m    553\u001b[0m     \"\"\"\n\u001b[0;32m    554\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shutil.copytree\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mitr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m         \u001b[0mentries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m     return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Le chemin d’accès spécifié est introuvable: './firefoxdriver'"
     ]
    }
   ],
   "source": [
    "for job in job_list:\n",
    "\n",
    "    opts = webdriver.FirefoxOptions()\n",
    "    opts.headless = True\n",
    "    profile = webdriver.FirefoxProfile('./firefoxdriver')\n",
    "    profile = webdriver.FirefoxProfile()\n",
    "    profile.set_preference(\"general.useragent.override\", \"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\")\n",
    "    driver = webdriver.Firefox(profile, executable_path=\"venv/bin/geckodriver\", options=opts) \n",
    "\n",
    "    driver.get(\n",
    "            \"https://www.indeed.fr/jobs?q=\" + job + \"&l=Paris+%2875%29&jt=fulltime&limit=50&radius=25&start=0\"\n",
    "        )\n",
    "\n",
    "    sleep(randint(7,10))\n",
    "    print('Collecting data for \"{}\"...' .format(job))\n",
    "    # First, get the number of jobs available\n",
    "    job_number = driver.find_element_by_xpath(\"//div[@id='searchCountPages']\").text\n",
    "    # Calculating number of pages to be crawled (number of jobs available - number of jobs per page (here, 30))\n",
    "    job_number = job_number.split(\" \", 4)\n",
    "    job_number = int(job_number[3])\n",
    "    print(\"- Number of open positions : {}\" .format(job_number))\n",
    "    exact_page_nb = job_number / 50\n",
    "    print(\"- Exact number of pages to be crawled : {}\" .format(exact_page_nb))\n",
    "    min_page_nb = job_number // 50\n",
    "    print(\"- Minimum number of pages to be crawled : {}\" .format(min_page_nb))\n",
    "\n",
    "    if exact_page_nb > min_page_nb:\n",
    "        page_nb = (min_page_nb) * 50\n",
    "        pages = [str(i) for i in range(0, page_nb, 50)]\n",
    "    elif exact_page_nb == min_page_nb:\n",
    "        page_nb = (min_page_nb - 1) * 50\n",
    "        pages = [str(i) for i in range(0, page_nb, 50)]\n",
    "\n",
    "    for page in pages:\n",
    "        driver.get(\n",
    "            \"https://www.indeed.fr/jobs?q=\" + job + \"&l=Paris+%2875%29&jt=fulltime&limit=50&radius=25&start=\" + page\n",
    "        )\n",
    "\n",
    "        sleep(randint(5, 12))\n",
    "\n",
    "        # Locating job container\n",
    "        all_cards = driver.find_elements_by_xpath(\"//div[@class='jobsearch-SerpJobCard unifiedRow row result clickcard']\")\n",
    "\n",
    "        for card in all_cards:\n",
    "\n",
    "            # Collecting job link\n",
    "            application_link = card.find_elements_by_css_selector('a')\n",
    "            if not application_link:\n",
    "                application_link = \"Unknown\"\n",
    "            else:\n",
    "                application_link = application_link[0].get_attribute('href')\n",
    "            application_links.append(application_link)\n",
    "\n",
    "            # Collecting job title\n",
    "            job_title = card.find_elements_by_css_selector('a')\n",
    "            if not job_title:\n",
    "                job_title = \"Unknown\"\n",
    "            else:\n",
    "                job_title = job_title[0].text\n",
    "            job_titles.append(job_title)\n",
    "\n",
    "            # Collecting company name\n",
    "            company_name = card.find_elements_by_css_selector('div.sjcl div span.company')\n",
    "            if not company_name:\n",
    "                company_name = \"Unknown\"\n",
    "            else:\n",
    "                company_name = company_name[0].text\n",
    "            company_names.append(company_name)\n",
    "            \n",
    "            # Collecting job location\n",
    "            job_location = card.find_elements_by_css_selector('.location.accessible-contrast-color-location')\n",
    "            if not job_location:\n",
    "                job_location = \"Unknown\"\n",
    "            else:\n",
    "                job_location = job_location[0].text\n",
    "            job_locations.append(job_location)\n",
    "            \n",
    "            # Collecting application type (easy apply)\n",
    "            application_type = card.find_elements_by_css_selector('.jobCardShelfContainer')\n",
    "            if not application_type:\n",
    "                application_type = \"company's website\"\n",
    "            else:\n",
    "                application_type = application_type[0].text\n",
    "            application_types.append(application_type) \n",
    "            \n",
    "            # Collecting publication date\n",
    "            publication_date = card.find_elements_by_css_selector('span.date')\n",
    "            if not publication_date:\n",
    "                publication_date = \"il y a 40 minutes\"\n",
    "            else:\n",
    "                publication_date = publication_date[0].text\n",
    "            publication_dates.append(publication_date)\n",
    "            \n",
    "            # Collecting generated scraping time \n",
    "            scraping_dates.append(datetime.now())\n",
    "\n",
    "    print('Crawling status for \"{}\" : Done' .format(job))\n",
    "    print()\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "print('Crawling time : {}' .format(datetime.now() - start_time))\n",
    "print('Dataframe successfuly created and exported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a4fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d34c1272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-3.8.5-py2.py3-none-any.whl (27 kB)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-0.21.1-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\msi\\anaconda3\\lib\\site-packages (from webdriver-manager) (4.59.0)\n",
      "Requirement already satisfied: requests in c:\\users\\msi\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.25.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\msi\\anaconda3\\lib\\site-packages (from webdriver-manager) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from packaging->webdriver-manager) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\msi\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2022.12.7)\n",
      "Installing collected packages: python-dotenv, webdriver-manager\n",
      "Successfully installed python-dotenv-0.21.1 webdriver-manager-3.8.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99064ae2",
   "metadata": {},
   "source": [
    "# Data Collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed1f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac4f95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"C:/Users/MSI/Desktop/Projet Data Science PI principal/Competency-based curriculum Platform-PI DATA SCIENCE/data/raw/BI_DATA\")\n",
    "df2=pd.read_csv(\"C:/Users/MSI/Desktop/Projet Data Science PI principal/Competency-based curriculum Platform-PI DATA SCIENCE/data/raw/Cloud-Data\")\n",
    "df3=pd.read_csv(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5816eb51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8f8d66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
